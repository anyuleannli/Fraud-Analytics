{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as sps \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.71 s, sys: 1.02 s, total: 7.72 s\n",
      "Wall time: 7.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data=pd.read_csv('data60.csv')\n",
    "oot=pd.read_csv('oot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_var=['fraud_label','Days_since_fulladdress', 'fulladdress30', 'fulladdress14', \n",
    "         'fulladdress7', 'fulladdress3', 'fulladdress1', 'Days_since_ssn', \n",
    "         'ssn30', 'Days_since_firstname_ssn', 'Days_since_lastname_ssn', \n",
    "         'Days_since_fulladdresshomephone', 'Days_since_nameDOB', 'ssnnameDOB30', \n",
    "         'Days_since_ssnnameDOB', 'fulladdresshomephone30', 'nameDOB30',\n",
    "         'lastnamessn30', 'firstnamessn30', 'fulladdresshomephone14',\n",
    "         'nameDOB14', 'ssnnameDOB14', 'fulladdresshomephone7', 'nameDOB7', \n",
    "         'ssn7', 'homephone3', 'homephone7','zip3_risk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data.loc[:, can_var]\n",
    "oot1=oot.loc[:,can_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>Days_since_fulladdress</th>\n",
       "      <th>fulladdress30</th>\n",
       "      <th>fulladdress14</th>\n",
       "      <th>fulladdress7</th>\n",
       "      <th>fulladdress3</th>\n",
       "      <th>fulladdress1</th>\n",
       "      <th>Days_since_ssn</th>\n",
       "      <th>ssn30</th>\n",
       "      <th>Days_since_firstname_ssn</th>\n",
       "      <th>...</th>\n",
       "      <th>firstnamessn30</th>\n",
       "      <th>fulladdresshomephone14</th>\n",
       "      <th>nameDOB14</th>\n",
       "      <th>ssnnameDOB14</th>\n",
       "      <th>fulladdresshomephone7</th>\n",
       "      <th>nameDOB7</th>\n",
       "      <th>ssn7</th>\n",
       "      <th>homephone3</th>\n",
       "      <th>homephone7</th>\n",
       "      <th>zip3_risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fraud_label  Days_since_fulladdress  fulladdress30  fulladdress14  \\\n",
       "0            0                     365              1              1   \n",
       "1            0                     365              1              1   \n",
       "2            0                     365              1              1   \n",
       "3            0                     365              1              1   \n",
       "4            0                     365              1              1   \n",
       "\n",
       "   fulladdress7  fulladdress3  fulladdress1  Days_since_ssn  ssn30  \\\n",
       "0             1             1             1             365      1   \n",
       "1             1             1             1             365      1   \n",
       "2             1             1             1             365      1   \n",
       "3             1             1             1             365      1   \n",
       "4             1             1             1             365      1   \n",
       "\n",
       "   Days_since_firstname_ssn    ...      firstnamessn30  \\\n",
       "0                       365    ...                   1   \n",
       "1                       365    ...                   1   \n",
       "2                       365    ...                   1   \n",
       "3                       365    ...                   1   \n",
       "4                       365    ...                   1   \n",
       "\n",
       "   fulladdresshomephone14  nameDOB14  ssnnameDOB14  fulladdresshomephone7  \\\n",
       "0                       1          1             1                      1   \n",
       "1                       1          1             1                      1   \n",
       "2                       1          1             1                      1   \n",
       "3                       1          1             1                      1   \n",
       "4                       1          1             1                      1   \n",
       "\n",
       "   nameDOB7  ssn7  homephone3  homephone7  zip3_risk  \n",
       "0         1     1           1           1    0.01252  \n",
       "1         1     1           1           1    0.01252  \n",
       "2         1     1           1           2    0.01252  \n",
       "3         1     1           1           1    0.01252  \n",
       "4         1     1           1           1    0.01252  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(\n",
    "    data1,\n",
    "    test_size=0.3,\n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2:all goods, data3:all bads\n",
    "data2 = train[train['fraud_label']==0]\n",
    "data3 = train[train['fraud_label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8377"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['fraud_label'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583454"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014566744975020736"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8377/(583454-8377)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for balanced dataset\n",
    "data4,data5 = train_test_split(\n",
    "    data2,\n",
    "    test_size=0.9,\n",
    "    random_state=1)\n",
    "data5 = data3.append(data4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8377"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5['fraud_label'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65884"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12714771416428874"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8377/65884"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8728522858357113"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(65884-8377)/65884"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14566922287721495"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8377/(65884-8377)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train = train.iloc[:,1:]\n",
    "y1_train = train.iloc[:,0]\n",
    "X1_test = test.iloc[:,1:]\n",
    "y1_test = test.iloc[:,0]\n",
    "X1_oot = oot1.iloc[:,1:]\n",
    "y1_oot = oot1.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(2, 3), random_state=1)\n",
    "nn.fit(X1_train, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "nn1_prob_test=nn.predict_proba(X1_test)\n",
    "nn_test_df = pd.DataFrame(nn1_prob_test,index=y1_test.index,columns = ['prob', 'y'])\n",
    "nn_test_df['y']=y1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "cur=0\n",
    "num=int(0.03*len(nn_test_df.index))\n",
    "for i in nn_test_df.sort_values(by='prob').index:\n",
    "    if cur<num:\n",
    "        cur+=1\n",
    "        k+=nn_test_df.loc[i,'y']\n",
    "# test\n",
    "k/sum(nn_test_df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "nn1_prob_train=nn.predict_proba(X1_train)\n",
    "nn_train_df = pd.DataFrame(nn1_prob_train,index=y1_train.index,columns = ['prob', 'y'])\n",
    "nn_train_df['y']=y1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "cur=0\n",
    "num=int(0.03*len(nn_train_df.index))\n",
    "for i in nn_train_df.sort_values(by='prob').index:\n",
    "    if cur<num:\n",
    "        cur+=1\n",
    "        k+=nn_train_df.loc[i,'y']\n",
    "# train\n",
    "10*k/sum(nn_train_df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oot\n",
    "nn1_prob_oot=nn.predict_proba(X1_oot)\n",
    "nn_oot_df = pd.DataFrame(nn1_prob_oot,index=y1_oot.index,columns = ['prob', 'y'])\n",
    "nn_oot_df['y']=y1_oot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "cur=0\n",
    "num=int(0.03*len(nn_oot_df.index))\n",
    "for i in nn_oot_df.sort_values(by='prob').index:\n",
    "    if cur<num:\n",
    "        cur+=1\n",
    "        k+=nn_oot_df.loc[i,'y']\n",
    "# oot FDR\n",
    "k/sum(nn_oot_df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDR_train_list={}\n",
    "FDR_test_list={}\n",
    "FDR_oot_list={}\n",
    "for s in range(0,10):\n",
    "    train, test = train_test_split(\n",
    "    data1,\n",
    "    test_size=0.3,\n",
    "    random_state=s)\n",
    "    X1_train = train.iloc[:,1:]\n",
    "    y1_train = train.iloc[:,0]\n",
    "    X1_test = test.iloc[:,1:]\n",
    "    y1_test = test.iloc[:,0]\n",
    "    X1_oot = oot1.iloc[:,1:]\n",
    "    y1_oot = oot1.iloc[:,0]\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    nn = MLPClassifier(activation='logistic',  batch_size='auto',\n",
    "            early_stopping=True, hidden_layer_sizes=(2,3), learning_rate='adaptive',\n",
    "            learning_rate_init=0.3, max_iter=1000, random_state=1,\n",
    "            solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "            warm_start=False)\n",
    "    nn.fit(X1_train, y1_train)\n",
    "    #train\n",
    "    nn1_prob_train=nn.predict_proba(X1_train)\n",
    "    nn_train_df = pd.DataFrame(nn1_prob_train,index=y1_train.index,columns = ['prob', 'y'])\n",
    "    nn_train_df['y']=y1_train\n",
    "    k_train=0\n",
    "    cur_train=0\n",
    "    num_train=int(0.03*len(nn_train_df.index))\n",
    "    for i in nn_train_df.sort_values(by='prob').index:\n",
    "        if cur_train<num_train:\n",
    "            cur_train+=1\n",
    "            k_train+=nn_train_df.loc[i,'y']\n",
    "    # FDR\n",
    "    FDR_train=k_train/sum(nn_train_df['y'])\n",
    "    #test\n",
    "    nn1_prob_test=nn.predict_proba(X1_test)\n",
    "    nn_test_df = pd.DataFrame(nn1_prob_test,index=y1_test.index,columns = ['prob', 'y'])\n",
    "    nn_test_df['y']=y1_test\n",
    "    #FDR\n",
    "    k_test=0\n",
    "    cur_test=0\n",
    "    num_test=int(0.03*len(nn_test_df.index))\n",
    "    for i in nn_test_df.sort_values(by='prob').index:\n",
    "        if cur_test<num_test:\n",
    "            cur_test+=1\n",
    "            k_test+=nn_test_df.loc[i,'y']\n",
    "    # test FDR\n",
    "    FDR_test=k_test/sum(nn_test_df['y'])\n",
    "    #OOT\n",
    "    nn1_prob_oot=nn.predict_proba(X1_oot)\n",
    "    nn_oot_df = pd.DataFrame(nn1_prob_oot,index=y1_oot.index,columns = ['prob', 'y'])\n",
    "    nn_oot_df['y']=y1_oot\n",
    "    #OOT FDR\n",
    "    k_oot=0\n",
    "    cur_oot=0\n",
    "    num_oot=int(0.03*len(nn_oot_df.index))\n",
    "    for i in nn_oot_df.sort_values(by='prob').index:\n",
    "        if cur_oot<num_oot:\n",
    "            cur_oot+=1\n",
    "            k_oot+=nn_oot_df.loc[i,'y']\n",
    "    # oot FDR\n",
    "    FDR_oot=k_oot/sum(nn_oot_df['y'])\n",
    "    FDR_train_list[s]=FDR_train\n",
    "    FDR_test_list[s]=FDR_test\n",
    "    FDR_oot_list[s]=FDR_oot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(2,3), random_state=1,activation='logistic',\n",
    "early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPClassifier(activation='logistic',  batch_size='auto',\n",
    "            early_stopping=True, hidden_layer_sizes=(2,3), learning_rate='adaptive',\n",
    "            learning_rate_init=0.1, max_iter=5000, random_state=1,\n",
    "            solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "            warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{0: 0.5718040621266428, 1: 0.5697826345171636, 2: 0.5434052757793765, 3: 0.5451940298507463, 4: 0.5375639957137754, 5: 0.5399621659966896, 6: 0.5557382826265711, 7: 0.5745489078822412, 8: 0.5371617629682146, 9: 0.5331264175719231}\n",
    "{0: 0.5680505911465493, 1: 0.568561872909699, 2: 0.5235887646577584, 3: 0.5412995594713657, 4: 0.5385254988913526, 5: 0.535643843336151, 6: 0.5592444190040069, 7: 0.574937203460787, 8: 0.5428893905191874, 9: 0.5564738292011019}\n",
    "{0: 0.5511316010058676, 1: 0.5452640402347024, 2: 0.5238893545683152, 3: 0.5284995808885163, 4: 0.5251466890192791, 5: 0.5255658005029338, 6: 0.5230511316010059, 7: 0.5586756077116513, 8: 0.5247275775356245, 9: 0.5243084660519698}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.515531660692951, 1: 0.526071980045136, 2: 0.5213429256594724, 3: 0.5204776119402985, 4: 0.5216097154423146, 5: 0.5115866635138331, 6: 0.5322448020674263, 7: 0.51661918328585, 8: 0.5130568356374808, 9: 0.510684015757431}\n",
      "{0: 0.5174594445971955, 1: 0.5256410256410257, 2: 0.5012271611671666, 3: 0.5134911894273128, 4: 0.5194013303769401, 5: 0.5085939701324317, 6: 0.5343445907269605, 7: 0.5166061959252023, 8: 0.5191873589164786, 9: 0.5377410468319559}\n",
      "{0: 0.4865884325230511, 1: 0.5067057837384744, 2: 0.4911986588432523, 3: 0.4911986588432523, 4: 0.4886839899413244, 5: 0.49077954735959767, 6: 0.5104777870913663, 7: 0.4853310980720872, 8: 0.49245599329421624, 9: 0.4979044425817267}\n"
     ]
    }
   ],
   "source": [
    "print(FDR_train_list)\n",
    "print(FDR_test_list)\n",
    "print(FDR_oot_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balanced train dataset\n",
    "FDR_train_balancedlist={}\n",
    "FDR_test_balancedlist={}\n",
    "FDR_oot_balancedlist={}\n",
    "for s in range(0,10):\n",
    "    train, test = train_test_split(\n",
    "    data1,\n",
    "    test_size=0.3,\n",
    "    random_state=s)\n",
    "    data2 = train[train['fraud_label']==0]\n",
    "    data3 = train[train['fraud_label']==1]\n",
    "    data4,data5 = train_test_split(\n",
    "    data2,\n",
    "    test_size=0.9,\n",
    "    random_state=s)\n",
    "    data5 = data3.append(data4) \n",
    "    X1_train = data5.iloc[:,1:]\n",
    "    y1_train = data5.iloc[:,0]\n",
    "    X1_test = test.iloc[:,1:]\n",
    "    y1_test = test.iloc[:,0]\n",
    "    X1_oot = oot1.iloc[:,1:]\n",
    "    y1_oot = oot1.iloc[:,0]\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    nn = MLPClassifier(activation='logistic',  batch_size='auto',\n",
    "            early_stopping=True, hidden_layer_sizes=(2,3), learning_rate='adaptive',\n",
    "            learning_rate_init=0.3, max_iter=1000, random_state=1,\n",
    "            solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "            warm_start=False)\n",
    "    nn.fit(X1_train, y1_train)\n",
    "    #train\n",
    "    nn1_prob_train=nn.predict_proba(X1_train)\n",
    "    nn_train_df = pd.DataFrame(nn1_prob_train,index=y1_train.index,columns = ['prob', 'y'])\n",
    "    nn_train_df['y']=y1_train\n",
    "    k_train=0\n",
    "    cur_train=0\n",
    "    num_train=int(0.03*len(train))\n",
    "    for i in nn_train_df.sort_values(by='prob').index:\n",
    "        if cur_train<num_train:\n",
    "            cur_train+=1\n",
    "            k_train+=nn_train_df.loc[i,'y']\n",
    "    # FDR\n",
    "    FDR_train=k_train/sum(nn_train_df['y'])\n",
    "    #test\n",
    "    nn1_prob_test=nn.predict_proba(X1_test)\n",
    "    nn_test_df = pd.DataFrame(nn1_prob_test,index=y1_test.index,columns = ['prob', 'y'])\n",
    "    nn_test_df['y']=y1_test\n",
    "    #FDR\n",
    "    k_test=0\n",
    "    cur_test=0\n",
    "    num_test=int(0.03*len(nn_test_df.index))\n",
    "    for i in nn_test_df.sort_values(by='prob').index:\n",
    "        if cur_test<num_test:\n",
    "            cur_test+=1\n",
    "            k_test+=nn_test_df.loc[i,'y']\n",
    "    # test FDR\n",
    "    FDR_test=k_test/sum(nn_test_df['y'])\n",
    "    #OOT\n",
    "    nn1_prob_oot=nn.predict_proba(X1_oot)\n",
    "    nn_oot_df = pd.DataFrame(nn1_prob_oot,index=y1_oot.index,columns = ['prob', 'y'])\n",
    "    nn_oot_df['y']=y1_oot\n",
    "    #OOT FDR\n",
    "    k_oot=0\n",
    "    cur_oot=0\n",
    "    num_oot=int(0.03*len(nn_oot_df.index))\n",
    "    for i in nn_oot_df.sort_values(by='prob').index:\n",
    "        if cur_oot<num_oot:\n",
    "            cur_oot+=1\n",
    "            k_oot+=nn_oot_df.loc[i,'y']\n",
    "    # oot FDR\n",
    "    FDR_oot=k_oot/sum(nn_oot_df['y'])\n",
    "    FDR_train_balancedlist[s]=FDR_train\n",
    "    FDR_test_balancedlist[s]=FDR_test\n",
    "    FDR_oot_balancedlist[s]=FDR_oot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5326164874551972, 1: 0.5219147167121986, 2: 0.5406474820143885, 3: 0.5364776119402985, 4: 0.6550779854744613, 5: 0.5244738708914637, 6: 0.5139198872312933, 7: 0.5274216524216524, 8: 0.5255819449367837, 9: 0.6516652739644264}\n",
      "{0: 0.5367060764366236, 1: 0.5278706800445931, 2: 0.5222252522497954, 3: 0.5324889867841409, 4: 0.5512749445676275, 5: 0.5291631445477599, 6: 0.5197481396680023, 7: 0.5333519397153224, 8: 0.5372460496613995, 9: 0.5611570247933885}\n",
      "{0: 0.5209555741827326, 1: 0.4945515507124895, 2: 0.5171835708298408, 3: 0.5217937971500419, 4: 0.5222129086336965, 5: 0.5163453478625314, 6: 0.49706621961441744, 7: 0.5138306789606035, 8: 0.510896898575021, 9: 0.5255658005029338}\n"
     ]
    }
   ],
   "source": [
    "print(FDR_train_balancedlist)\n",
    "print(FDR_test_balancedlist)\n",
    "print(FDR_oot_balancedlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run for loop to test parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_oot={}\n",
    "train, test = train_test_split(\n",
    "data1,\n",
    "test_size=0.3,\n",
    "random_state=1)\n",
    "data2 = train[train['fraud_label']==0]\n",
    "data3 = train[train['fraud_label']==1]\n",
    "data4,data5 = train_test_split(\n",
    "data2,\n",
    "test_size=0.9,\n",
    "random_state=1)\n",
    "data5 = data3.append(data4) \n",
    "X1_train = data5.iloc[:,1:]\n",
    "y1_train = data5.iloc[:,0]\n",
    "X1_test = test.iloc[:,1:]\n",
    "y1_test = test.iloc[:,0]\n",
    "X1_oot = oot1.iloc[:,1:]\n",
    "y1_oot = oot1.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0.02388935456831517\n",
      "1 2 0.02388935456831517\n",
      "1 3 0.18943839061190276\n",
      "1 4 0.02388935456831517\n",
      "1 5 0.11106454316848281\n",
      "1 6 0.3616932103939648\n",
      "1 7 0.02388935456831517\n",
      "1 8 0.02388935456831517\n",
      "1 9 0.02388935456831517\n",
      "1 10 0.02388935456831517\n",
      "1 11 0.02388935456831517\n",
      "1 12 0.02388935456831517\n",
      "1 13 0.20368818105616093\n",
      "1 14 0.2355406538139145\n",
      "1 15 0.02388935456831517\n",
      "1 16 0.02388935456831517\n",
      "1 17 0.10980720871751885\n",
      "1 18 0.02388935456831517\n",
      "1 19 0.02388935456831517\n",
      "1 20 0.02388935456831517\n",
      "1 21 0.02388935456831517\n",
      "1 22 0.02388935456831517\n",
      "1 23 0.02388935456831517\n",
      "1 24 0.02388935456831517\n",
      "1 25 0.02388935456831517\n",
      "2 1 0.30511316010058676\n",
      "2 2 0.4878457669740151\n",
      "2 3 0.5167644593461861\n",
      "2 4 0.4782062028499581\n",
      "2 5 0.3721709974853311\n",
      "2 6 0.02388935456831517\n",
      "2 7 0.4865884325230511\n",
      "2 8 0.5134115674769488\n",
      "2 9 0.45892707460184406\n",
      "2 10 0.5117351215423303\n",
      "2 11 0.3688181056160939\n",
      "2 12 0.48113998323554064\n",
      "2 13 0.5025146689019279\n",
      "2 14 0.02388935456831517\n",
      "2 15 0.35875943000838223\n",
      "2 16 0.3608549874266555\n",
      "2 17 0.4899413243922883\n",
      "2 18 0.36378876781223807\n",
      "2 19 0.5159262363788768\n",
      "2 20 0.23344509639564123\n",
      "2 21 0.5079631181894384\n",
      "2 22 0.5209555741827326\n",
      "2 23 0.5150880134115675\n",
      "2 24 0.3642078792958927\n",
      "2 25 0.5079631181894384\n",
      "3 1 0.2707460184409053\n",
      "3 2 0.3629505448449287\n",
      "3 3 0.11022632020117351\n",
      "3 4 0.46563285834031853\n",
      "3 5 0.02388935456831517\n",
      "3 6 0.3671416596814753\n",
      "3 7 0.5138306789606035\n",
      "3 8 0.02388935456831517\n",
      "3 9 0.490360435875943\n",
      "3 10 0.4823973176865046\n",
      "3 11 0.30972338642078795\n",
      "3 12 0.5171835708298408\n",
      "3 13 0.3193629505448449\n",
      "3 14 0.2967309304274937\n",
      "3 15 0.4911986588432523\n",
      "3 16 0.5301760268231349\n",
      "3 17 0.02388935456831517\n",
      "3 18 0.11399832355406538\n",
      "3 19 0.49245599329421624\n",
      "3 20 0.5247275775356245\n",
      "3 21 0.49832355406538137\n",
      "3 22 0.49958088851634536\n",
      "3 23 0.36462699077954736\n",
      "3 24 0.02388935456831517\n",
      "3 25 0.11777032690695725\n",
      "4 1 0.5142497904442582\n",
      "4 2 0.4430008382229673\n",
      "4 3 0.480720871751886\n",
      "4 4 0.5100586756077117\n",
      "4 5 0.024308466051969825\n",
      "4 6 0.3642078792958927\n",
      "4 7 0.49874266554903607\n",
      "4 8 0.5159262363788768\n",
      "4 9 0.4798826487845767\n",
      "4 10 0.4333612740989103\n",
      "4 11 0.11316010058675607\n",
      "4 12 0.5201173512154234\n",
      "4 13 0.44216261525565803\n",
      "4 14 0.02388935456831517\n",
      "4 15 0.5092204526404024\n",
      "4 16 0.4966471081307628\n",
      "4 17 0.4966471081307628\n",
      "4 18 0.48910310142497904\n",
      "4 19 0.4895222129086337\n",
      "4 20 0.36797988264878456\n",
      "4 21 0.46814752724224645\n",
      "4 22 0.512154233025985\n",
      "4 23 0.43629505448449285\n",
      "4 24 0.5058675607711651\n",
      "4 25 0.49161777032690696\n",
      "5 1 0.02388935456831517\n",
      "5 2 0.02388935456831517\n",
      "5 3 0.49245599329421624\n",
      "5 4 0.393964794635373\n",
      "5 5 0.48575020955574183\n",
      "5 6 0.45934618608549876\n",
      "5 7 0.4861693210393965\n",
      "5 8 0.28918692372170995\n",
      "5 9 0.41366303436714164\n",
      "5 10 0.47611064543168485\n",
      "5 11 0.5113160100586757\n",
      "5 12 0.3784576697401509\n",
      "5 13 0.4673093042749371\n",
      "5 14 0.42875104777870915\n",
      "5 15 0.48742665549036046\n",
      "5 16 0.4694048616932104\n",
      "5 17 0.20871751886001677\n",
      "5 18 0.5138306789606035\n",
      "5 19 0.5134115674769488\n",
      "5 20 0.48742665549036046\n",
      "5 21 0.5079631181894384\n",
      "5 22 0.48197820620285\n",
      "5 23 0.48113998323554064\n",
      "5 24 0.3319362950544845\n",
      "5 25 0.4853310980720872\n",
      "6 1 0.02388935456831517\n",
      "6 2 0.02388935456831517\n",
      "6 3 0.4002514668901928\n",
      "6 4 0.48365465213746855\n",
      "6 5 0.4928751047778709\n",
      "6 6 0.49371332774518023\n",
      "6 7 0.49832355406538137\n",
      "6 8 0.3034367141659681\n",
      "6 9 0.41533948030176027\n",
      "6 10 0.4752724224643755\n",
      "6 11 0.48491198658843254\n",
      "6 12 0.4237217099748533\n",
      "6 13 0.45347862531433364\n",
      "6 14 0.4974853310980721\n",
      "6 15 0.49077954735959767\n",
      "6 16 0.5079631181894384\n",
      "6 17 0.5196982397317687\n",
      "6 18 0.4974853310980721\n",
      "6 19 0.4316848281642917\n",
      "6 20 0.46186085498742663\n",
      "6 21 0.48491198658843254\n",
      "6 22 0.49497066219614416\n",
      "6 23 0.501257334450964\n",
      "6 24 0.5088013411567477\n",
      "6 25 0.4899413243922883\n",
      "7 1 0.4932942162615256\n",
      "7 2 0.02388935456831517\n",
      "7 3 0.08549874266554904\n",
      "7 4 0.23889354568315171\n",
      "7 5 0.5020955574182733\n",
      "7 6 0.23260687342833195\n",
      "7 7 0.41114836546521377\n",
      "7 8 0.49245599329421624\n",
      "7 9 0.3214585079631182\n",
      "7 10 0.48700754400670576\n",
      "7 11 0.4974853310980721\n",
      "7 12 0.47611064543168485\n",
      "7 13 0.508382229673093\n",
      "7 14 0.5033528918692373\n",
      "7 15 0.4945515507124895\n",
      "7 16 0.4895222129086337\n",
      "7 17 0.49161777032690696\n",
      "7 18 0.49245599329421624\n",
      "7 19 0.4958088851634535\n",
      "7 20 0.4790444258172674\n",
      "7 21 0.3948030176026823\n",
      "7 22 0.48365465213746855\n",
      "7 23 0.4911986588432523\n",
      "7 24 0.5234702430846605\n",
      "7 25 0.32858340318524726\n",
      "8 1 0.5331098072087175\n",
      "8 2 0.41533948030176027\n",
      "8 3 0.501257334450964\n",
      "8 4 0.49622799664710815\n",
      "8 5 0.4861693210393965\n",
      "8 6 0.4178541492036882\n",
      "8 7 0.4790444258172674\n",
      "8 8 0.5092204526404024\n",
      "8 9 0.43755238893545684\n",
      "8 10 0.4911986588432523\n",
      "8 11 0.5008382229673093\n",
      "8 12 0.4966471081307628\n",
      "8 13 0.4832355406538139\n",
      "8 14 0.4832355406538139\n",
      "8 15 0.39606035205364626\n",
      "8 16 0.18692372170997484\n",
      "8 17 0.08591785414920369\n",
      "8 18 0.5238893545683152\n",
      "8 19 0.5037720033528919\n",
      "8 20 0.4790444258172674\n",
      "8 21 0.3101424979044426\n",
      "8 22 0.48742665549036046\n",
      "8 23 0.5079631181894384\n",
      "8 24 0.48910310142497904\n",
      "8 25 0.48365465213746855\n",
      "9 1 0.5029337803855826\n",
      "9 2 0.539815590947192\n",
      "9 3 0.49832355406538137\n",
      "9 4 0.3516345347862531\n",
      "9 5 0.4953897736797988\n",
      "9 6 0.1366303436714166\n",
      "9 7 0.4861693210393965\n",
      "9 8 0.5159262363788768\n",
      "9 9 0.5046102263202011\n",
      "9 10 0.4786253143336127\n",
      "9 11 0.48491198658843254\n",
      "9 12 0.23218776194467727\n",
      "9 13 0.2661357921207041\n",
      "9 14 0.17476948868398995\n",
      "9 15 0.47150041911148366\n",
      "9 16 0.48365465213746855\n",
      "9 17 0.48700754400670576\n",
      "9 18 0.18860016764459347\n",
      "9 19 0.5301760268231349\n",
      "9 20 0.5041911148365466\n",
      "9 21 0.47359597652975693\n",
      "9 22 0.48910310142497904\n",
      "9 23 0.510896898575021\n",
      "9 24 0.5016764459346186\n",
      "9 25 0.03143336127409891\n",
      "10 1 0.4270746018440905\n",
      "10 2 0.4945515507124895\n",
      "10 3 0.49371332774518023\n",
      "10 4 0.5393964794635373\n",
      "10 5 0.44970662196144173\n",
      "10 6 0.46479463537300925\n",
      "10 7 0.4484492875104778\n",
      "10 8 0.46982397317686503\n",
      "10 9 0.5222129086336965\n",
      "10 10 0.4945515507124895\n",
      "10 11 0.4559932942162615\n",
      "10 12 0.4844928751047779\n",
      "10 13 0.5142497904442582\n",
      "10 14 0.2292539815590947\n",
      "10 15 0.4953897736797988\n",
      "10 16 0.4832355406538139\n",
      "10 17 0.4790444258172674\n",
      "10 18 0.4823973176865046\n",
      "10 19 0.5046102263202011\n",
      "10 20 0.4920368818105616\n",
      "10 21 0.5029337803855826\n",
      "10 22 0.22129086336965634\n",
      "10 23 0.17979882648784576\n",
      "10 24 0.47066219614417437\n",
      "10 25 0.4459346186085499\n",
      "11 1 0.4974853310980721\n",
      "11 2 0.02388935456831517\n",
      "11 3 0.5226320201173512\n",
      "11 4 0.5058675607711651\n",
      "11 5 0.02347024308466052\n",
      "11 6 0.4308466051969824\n",
      "11 7 0.49371332774518023\n",
      "11 8 0.49371332774518023\n",
      "11 9 0.48700754400670576\n",
      "11 10 0.43419949706621963\n",
      "11 11 0.4932942162615256\n",
      "11 12 0.039815590947191955\n",
      "11 13 0.46898575020955574\n",
      "11 14 0.4861693210393965\n",
      "11 15 0.2606873428331936\n",
      "11 16 0.48113998323554064\n",
      "11 17 0.48197820620285\n",
      "11 18 0.49497066219614416\n",
      "11 19 0.4941324392288349\n",
      "11 20 0.4958088851634535\n",
      "11 21 0.5155071248952221\n",
      "11 22 0.4886839899413244\n",
      "11 23 0.3046940486169321\n",
      "11 24 0.4932942162615256\n",
      "11 25 0.5079631181894384\n",
      "12 1 0.02347024308466052\n",
      "12 2 0.5062866722548198\n",
      "12 3 0.5054484492875104\n",
      "12 4 0.5176026823134954\n",
      "12 5 0.512154233025985\n",
      "12 6 0.5037720033528919\n",
      "12 7 0.48113998323554064\n",
      "12 8 0.48491198658843254\n",
      "12 9 0.27451802179379714\n",
      "12 10 0.48365465213746855\n",
      "12 11 0.49245599329421624\n",
      "12 12 0.4899413243922883\n",
      "12 13 0.09304274937133278\n",
      "12 14 0.27284157585917856\n",
      "12 15 0.4941324392288349\n",
      "12 16 0.4245599329421626\n",
      "12 17 0.49706621961441744\n",
      "12 18 0.5046102263202011\n",
      "12 19 0.3629505448449287\n",
      "12 20 0.23595976529756915\n",
      "12 21 0.2196144174350377\n",
      "12 22 0.3109807208717519\n",
      "12 23 0.07795473595976529\n",
      "12 24 0.48197820620285\n",
      "12 25 0.5092204526404024\n",
      "13 1 0.49706621961441744\n",
      "13 2 0.47359597652975693\n",
      "13 3 0.02388935456831517\n",
      "13 4 0.02388935456831517\n",
      "13 5 0.4291701592623638\n",
      "13 6 0.4660519698239732\n",
      "13 7 0.4932942162615256\n",
      "13 8 0.2606873428331936\n",
      "13 9 0.5104777870913663\n",
      "13 10 0.49958088851634536\n",
      "13 11 0.5075440067057837\n",
      "13 12 0.4974853310980721\n",
      "13 13 0.48826487845766975\n",
      "13 14 0.5033528918692373\n",
      "13 15 0.5092204526404024\n",
      "13 16 0.25020955574182735\n",
      "13 17 0.5016764459346186\n",
      "13 18 0.4459346186085499\n",
      "13 19 0.5058675607711651\n",
      "13 20 0.4538977367979883\n",
      "13 21 0.5033528918692373\n",
      "13 22 0.5\n",
      "13 23 0.5004191114836547\n",
      "13 24 0.32439228834870076\n",
      "13 25 0.5041911148365466\n",
      "14 1 0.02388935456831517\n",
      "14 2 0.02388935456831517\n",
      "14 3 0.49832355406538137\n",
      "14 4 0.02388935456831517\n",
      "14 5 0.5033528918692373\n",
      "14 6 0.47946353730092206\n",
      "14 7 0.42623637887678123\n",
      "14 8 0.11567476948868399\n",
      "14 9 0.4765297569153395\n",
      "14 10 0.5037720033528919\n",
      "14 11 0.4941324392288349\n",
      "14 12 0.5134115674769488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 13 0.4953897736797988\n",
      "14 14 0.21207041072925398\n",
      "14 15 0.4782062028499581\n",
      "14 16 0.48197820620285\n",
      "14 17 0.48113998323554064\n",
      "14 18 0.510896898575021\n",
      "14 19 0.2933780385582565\n",
      "14 20 0.5050293378038558\n",
      "14 21 0.28331936295054483\n",
      "14 22 0.49706621961441744\n",
      "14 23 0.2531433361274099\n",
      "14 24 0.5\n",
      "14 25 0.508382229673093\n",
      "15 1 0.02388935456831517\n",
      "15 2 0.510896898575021\n",
      "15 3 0.02388935456831517\n",
      "15 4 0.4974853310980721\n",
      "15 5 0.48575020955574183\n",
      "15 6 0.49874266554903607\n",
      "15 7 0.5163453478625314\n",
      "15 8 0.42875104777870915\n",
      "15 9 0.49161777032690696\n",
      "15 10 0.4974853310980721\n",
      "15 11 0.450544844928751\n",
      "15 12 0.45473595976529757\n",
      "15 13 0.32984073763621125\n",
      "15 14 0.4316848281642917\n",
      "15 15 0.432523051131601\n",
      "15 16 0.433780385582565\n",
      "15 17 0.43126571668063707\n",
      "15 18 0.5176026823134954\n",
      "15 19 0.4404861693210394\n",
      "15 20 0.4832355406538139\n",
      "15 21 0.45892707460184406\n",
      "15 22 0.5075440067057837\n",
      "15 23 0.06496227996647108\n",
      "15 24 0.4920368818105616\n",
      "15 25 0.055322715842414084\n",
      "16 1 0.02388935456831517\n",
      "16 2 0.5004191114836547\n",
      "16 3 0.02347024308466052\n",
      "16 4 0.5050293378038558\n",
      "16 5 0.5167644593461861\n",
      "16 6 0.48575020955574183\n",
      "16 7 0.48742665549036046\n",
      "16 8 0.4878457669740151\n",
      "16 9 0.4639564124056999\n",
      "16 10 0.49874266554903607\n",
      "16 11 0.4932942162615256\n",
      "16 12 0.5092204526404024\n",
      "16 13 0.5079631181894384\n",
      "16 14 0.46689019279128247\n",
      "16 15 0.49497066219614416\n",
      "16 16 0.5050293378038558\n",
      "16 17 0.5004191114836547\n",
      "16 18 0.5029337803855826\n",
      "16 19 0.4597652975691534\n",
      "16 20 0.48030176026823135\n",
      "16 21 0.028080469404861693\n",
      "16 22 0.5088013411567477\n",
      "16 23 0.5238893545683152\n",
      "16 24 0.4765297569153395\n",
      "16 25 0.49832355406538137\n",
      "17 1 0.44970662196144173\n",
      "17 2 0.02388935456831517\n",
      "17 3 0.5071248952221291\n",
      "17 4 0.02388935456831517\n",
      "17 5 0.36797988264878456\n",
      "17 6 0.02388935456831517\n",
      "17 7 0.48826487845766975\n",
      "17 8 0.1978206202849958\n",
      "17 9 0.4765297569153395\n",
      "17 10 0.44174350377200333\n",
      "17 11 0.48113998323554064\n",
      "17 12 0.5129924559932942\n",
      "17 13 0.49161777032690696\n",
      "17 14 0.47485331098072087\n",
      "17 15 0.4932942162615256\n",
      "17 16 0.4702430846605197\n",
      "17 17 0.4719195305951383\n",
      "17 18 0.501257334450964\n",
      "17 19 0.28667225481978204\n",
      "17 20 0.5062866722548198\n",
      "17 21 0.384325230511316\n",
      "17 22 0.45222129086336965\n",
      "17 23 0.21332774518021794\n",
      "17 24 0.49622799664710815\n",
      "17 25 0.47485331098072087\n",
      "18 1 0.4886839899413244\n",
      "18 2 0.5079631181894384\n",
      "18 3 0.49622799664710815\n",
      "18 4 0.5037720033528919\n",
      "18 5 0.21500419111483654\n",
      "18 6 0.45305951383067894\n",
      "18 7 0.4966471081307628\n",
      "18 8 0.5117351215423303\n",
      "18 9 0.4899413243922883\n",
      "18 10 0.4840737636211232\n",
      "18 11 0.5033528918692373\n",
      "18 12 0.5\n",
      "18 13 0.5058675607711651\n",
      "18 14 0.1978206202849958\n",
      "18 15 0.490360435875943\n",
      "18 16 0.45138306789606036\n",
      "18 17 0.5071248952221291\n",
      "18 18 0.5075440067057837\n",
      "18 19 0.18860016764459347\n",
      "18 20 0.5029337803855826\n",
      "18 21 0.5004191114836547\n",
      "18 22 0.06244761106454317\n",
      "18 23 0.5050293378038558\n",
      "18 24 0.49161777032690696\n",
      "18 25 0.287929589270746\n",
      "19 1 0.02388935456831517\n",
      "19 2 0.4052808046940486\n",
      "19 3 0.4157585917854149\n",
      "19 4 0.490360435875943\n",
      "19 5 0.2594300083822297\n",
      "19 6 0.5008382229673093\n",
      "19 7 0.5259849119865885\n",
      "19 8 0.4991617770326907\n",
      "19 9 0.17099748533109807\n",
      "19 10 0.5071248952221291\n",
      "19 11 0.38809723386420786\n",
      "19 12 0.5025146689019279\n",
      "19 13 0.490360435875943\n",
      "19 14 0.3776194467728416\n",
      "19 15 0.49706621961441744\n",
      "19 16 0.510896898575021\n",
      "19 17 0.26110645431684826\n",
      "19 18 0.5008382229673093\n",
      "19 19 0.5020955574182733\n",
      "19 20 0.47066219614417437\n",
      "19 21 0.47569153394803015\n",
      "19 22 0.5088013411567477\n",
      "19 23 0.48742665549036046\n",
      "19 24 0.5125733445096395\n",
      "19 25 0.4966471081307628\n",
      "20 1 0.02388935456831517\n",
      "20 2 0.02388935456831517\n",
      "20 3 0.5062866722548198\n",
      "20 4 0.5343671416596815\n",
      "20 5 0.5297569153394803\n",
      "20 6 0.2908633696563286\n",
      "20 7 0.49622799664710815\n",
      "20 8 0.4953897736797988\n",
      "20 9 0.29631181894383907\n",
      "20 10 0.4974853310980721\n",
      "20 11 0.5033528918692373\n",
      "20 12 0.3989941324392288\n",
      "20 13 0.509639564124057\n",
      "20 14 0.5041911148365466\n",
      "20 15 0.5037720033528919\n",
      "20 16 0.4966471081307628\n",
      "20 17 0.4832355406538139\n",
      "20 18 0.4744341994970662\n",
      "20 19 0.5037720033528919\n",
      "20 20 0.4941324392288349\n",
      "20 21 0.49077954735959767\n",
      "20 22 0.4580888516345348\n",
      "20 23 0.5008382229673093\n",
      "20 24 0.4966471081307628\n",
      "20 25 0.5050293378038558\n",
      "21 1 0.43419949706621963\n",
      "21 2 0.5004191114836547\n",
      "21 3 0.2954735959765298\n",
      "21 4 0.4090528080469405\n",
      "21 5 0.5180217937971501\n",
      "21 6 0.5\n",
      "21 7 0.49832355406538137\n",
      "21 8 0.4920368818105616\n",
      "21 9 0.5008382229673093\n",
      "21 10 0.4840737636211232\n",
      "21 11 0.5029337803855826\n",
      "21 12 0.46353730092204526\n",
      "21 13 0.5\n",
      "21 14 0.47485331098072087\n",
      "21 15 0.47611064543168485\n",
      "21 16 0.4430008382229673\n",
      "21 17 0.49161777032690696\n",
      "21 18 0.501257334450964\n",
      "21 19 0.501257334450964\n",
      "21 20 0.45347862531433364\n",
      "21 21 0.5075440067057837\n",
      "21 22 0.47233864207879295\n",
      "21 23 0.2644593461860855\n",
      "21 24 0.47066219614417437\n",
      "21 25 0.5020955574182733\n",
      "22 1 0.02388935456831517\n",
      "22 2 0.5075440067057837\n",
      "22 3 0.5050293378038558\n",
      "22 4 0.5020955574182733\n",
      "22 5 0.510896898575021\n",
      "22 6 0.4895222129086337\n",
      "22 7 0.4840737636211232\n",
      "22 8 0.4840737636211232\n",
      "22 9 0.1785414920368818\n",
      "22 10 0.5033528918692373\n",
      "22 11 0.4383906119027661\n",
      "22 12 0.5020955574182733\n",
      "22 13 0.3629505448449287\n",
      "22 14 0.5046102263202011\n",
      "22 15 0.49874266554903607\n",
      "22 16 0.5188600167644594\n",
      "22 17 0.5029337803855826\n",
      "22 18 0.49706621961441744\n",
      "22 19 0.39354568315171834\n",
      "22 20 0.4941324392288349\n",
      "22 21 0.4974853310980721\n",
      "22 22 0.2640402347024308\n",
      "22 23 0.5\n",
      "22 24 0.4932942162615256\n",
      "22 25 0.48826487845766975\n",
      "23 1 0.3516345347862531\n",
      "23 2 0.02388935456831517\n",
      "23 3 0.46186085498742663\n",
      "23 4 0.26236378876781224\n",
      "23 5 0.5054484492875104\n",
      "23 6 0.5004191114836547\n",
      "23 7 0.4966471081307628\n",
      "23 8 0.49245599329421624\n",
      "23 9 0.5067057837384744\n",
      "23 10 0.48742665549036046\n",
      "23 11 0.46814752724224645\n",
      "23 12 0.5150880134115675\n",
      "23 13 0.4744341994970662\n",
      "23 14 0.4509639564124057\n",
      "23 15 0.4631181894383906\n",
      "23 16 0.2862531433361274\n",
      "23 17 0.4853310980720872\n",
      "23 18 0.37594300083822296\n",
      "23 19 0.2518860016764459\n",
      "23 20 0.4832355406538139\n",
      "23 21 0.34283319362950543\n",
      "23 22 0.480720871751886\n",
      "23 23 0.09220452640402348\n",
      "23 24 0.2443419949706622\n",
      "23 25 0.43755238893545684\n",
      "24 1 0.5037720033528919\n",
      "24 2 0.5041911148365466\n",
      "24 3 0.5029337803855826\n",
      "24 4 0.40737636211232187\n",
      "24 5 0.5016764459346186\n",
      "24 6 0.2292539815590947\n",
      "24 7 0.4974853310980721\n",
      "24 8 0.44886839899413244\n",
      "24 9 0.31978206202849957\n",
      "24 10 0.5075440067057837\n",
      "24 11 0.48826487845766975\n",
      "24 12 0.44006705783738476\n",
      "24 13 0.4459346186085499\n",
      "24 14 0.41827326068734283\n",
      "24 15 0.5050293378038558\n",
      "24 16 0.4865884325230511\n",
      "24 17 0.5067057837384744\n",
      "24 18 0.04735959765297569\n",
      "24 19 0.4815590947191953\n",
      "24 20 0.49706621961441744\n",
      "24 21 0.4463537300922045\n",
      "24 22 0.508382229673093\n",
      "24 23 0.5134115674769488\n",
      "24 24 0.41408214585079633\n",
      "24 25 0.5092204526404024\n",
      "25 1 0.02388935456831517\n",
      "25 2 0.40611902766135793\n",
      "25 3 0.49706621961441744\n",
      "25 4 0.4920368818105616\n",
      "25 5 0.5046102263202011\n",
      "25 6 0.4945515507124895\n",
      "25 7 0.26571668063704945\n",
      "25 8 0.4270746018440905\n",
      "25 9 0.33780385582564965\n",
      "25 10 0.4124056999161777\n",
      "25 11 0.5062866722548198\n",
      "25 12 0.2564962279966471\n",
      "25 13 0.365046102263202\n",
      "25 14 0.49958088851634536\n",
      "25 15 0.5092204526404024\n",
      "25 16 0.48197820620285\n",
      "25 17 0.26697401508801344\n",
      "25 18 0.48826487845766975\n",
      "25 19 0.49958088851634536\n",
      "25 20 0.46186085498742663\n",
      "25 21 0.4572506286672255\n",
      "25 22 0.4639564124056999\n",
      "25 23 0.4911986588432523\n",
      "25 24 0.5067057837384744\n",
      "25 25 0.4786253143336127\n"
     ]
    }
   ],
   "source": [
    "for j in range(1,26):\n",
    "    for k in range(1,26):\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        nn = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(j,k), random_state=1)\n",
    "        nn.fit(X1_train, y1_train)\n",
    "        #OOT\n",
    "        nn1_prob_oot=nn.predict_proba(X1_oot)\n",
    "        nn_oot_df = pd.DataFrame(nn1_prob_oot,index=y1_oot.index,columns = ['prob', 'y'])\n",
    "        nn_oot_df['y']=y1_oot\n",
    "        #OOT FDR\n",
    "        k_oot=0\n",
    "        cur_oot=0\n",
    "        num_oot=int(0.03*len(nn_oot_df.index))\n",
    "        for i in nn_oot_df.sort_values(by='prob').index:\n",
    "            if cur_oot<num_oot:\n",
    "                cur_oot+=1\n",
    "                k_oot+=nn_oot_df.loc[i,'y']\n",
    "        # oot FDR\n",
    "        fdr=k_oot/sum(nn_oot_df['y'])\n",
    "        print(j,k,fdr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.29044425817267394\n",
      "2 0.5037720033528919\n",
      "3 0.3583403185247276\n",
      "4 0.5071248952221291\n",
      "5 0.3683989941324392\n",
      "6 0.5150880134115675\n",
      "7 0.5180217937971501\n",
      "8 0.5167644593461861\n",
      "9 0.5431684828164292\n",
      "10 0.4932942162615256\n",
      "11 0.5230511316010059\n",
      "12 0.5159262363788768\n",
      "13 0.5381391450125733\n",
      "14 0.528918692372171\n",
      "15 0.5565800502933781\n",
      "16 0.5565800502933781\n",
      "17 0.528918692372171\n",
      "18 0.5561609388097234\n",
      "19 0.5553227158424141\n",
      "20 0.5515507124895223\n",
      "21 0.5553227158424141\n",
      "22 0.548197820620285\n",
      "23 0.5477787091366303\n",
      "24 0.5523889354568315\n",
      "25 0.5490360435875943\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "for k in range(1,26):\n",
    "    nn = MLPClassifier(activation='logistic',  batch_size='auto',\n",
    "            early_stopping=True, hidden_layer_sizes=(k,), learning_rate='adaptive',\n",
    "            learning_rate_init=0.3, max_iter=1000, random_state=1,\n",
    "            solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "            warm_start=False)\n",
    "    nn.fit(X1_train, y1_train)\n",
    "    #OOT\n",
    "    nn1_prob_oot=nn.predict_proba(X1_oot)\n",
    "    nn_oot_df = pd.DataFrame(nn1_prob_oot,index=y1_oot.index,columns = ['prob', 'y'])\n",
    "    nn_oot_df['y']=y1_oot\n",
    "    #OOT FDR\n",
    "    k_oot=0\n",
    "    cur_oot=0\n",
    "    num_oot=int(0.03*len(nn_oot_df.index))\n",
    "    for i in nn_oot_df.sort_values(by='prob').index:\n",
    "        if cur_oot<num_oot:\n",
    "            cur_oot+=1\n",
    "            k_oot+=nn_oot_df.loc[i,'y']\n",
    "    # oot FDR\n",
    "    fdr=k_oot/sum(nn_oot_df['y'])\n",
    "    print(k,fdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 1 0.02388935456831517\n",
      "39 2 0.02388935456831517\n",
      "39 3 0.02388935456831517\n",
      "39 4 0.32606873428331934\n",
      "39 5 0.4543168482816429\n",
      "39 6 0.4878457669740151\n",
      "39 7 0.4371332774518022\n",
      "39 8 0.20955574182732606\n",
      "39 9 0.46186085498742663\n",
      "39 10 0.4559932942162615\n",
      "39 11 0.24056999161777032\n",
      "39 12 0.3348700754400671\n",
      "39 13 0.4861693210393965\n",
      "39 14 0.4974853310980721\n",
      "39 15 0.49832355406538137\n",
      "39 16 0.49497066219614416\n",
      "39 17 0.49832355406538137\n",
      "39 18 0.2585917854149204\n",
      "39 19 0.4966471081307628\n",
      "39 20 0.5004191114836547\n",
      "39 21 0.5008382229673093\n",
      "39 22 0.5037720033528919\n",
      "39 23 0.49077954735959767\n",
      "39 24 0.5020955574182733\n",
      "39 25 0.24685666387259012\n"
     ]
    }
   ],
   "source": [
    "for k in range(1,26):\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    nn = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(39,k), random_state=1)\n",
    "    nn.fit(X1_train, y1_train)\n",
    "    #OOT\n",
    "    nn1_prob_oot=nn.predict_proba(X1_oot)\n",
    "    nn_oot_df = pd.DataFrame(nn1_prob_oot,index=y1_oot.index,columns = ['prob', 'y'])\n",
    "    nn_oot_df['y']=y1_oot\n",
    "    #OOT FDR\n",
    "    k_oot=0\n",
    "    cur_oot=0\n",
    "    num_oot=int(0.03*len(nn_oot_df.index))\n",
    "    for i in nn_oot_df.sort_values(by='prob').index:\n",
    "        if cur_oot<num_oot:\n",
    "            cur_oot+=1\n",
    "            k_oot+=nn_oot_df.loc[i,'y']\n",
    "    # oot FDR\n",
    "    fdr=k_oot/sum(nn_oot_df['y'])\n",
    "    print(39,k,fdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
